{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4cd6ca6e",
   "metadata": {},
   "source": [
    "# Entraînement d'un CNN avec des images de spectrogrammes\n",
    "\n",
    "Ce notebook entraîne un réseau de neurones convolutionnel (CNN) en utilisant des images de spectrogrammes des vibrations du moteur. Le jeu de données est divisé en ensembles d'entraînement, de validation et de test. Le modèle entraîné est sauvegardé avec un horodatage et une convention de nommage basée sur les performances."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f09fe9",
   "metadata": {},
   "source": [
    "## Importation des bibliothèques (libraries) nécessaires\n",
    "\n",
    "Nous allons importer les bibliothèques nécessaires pour le chargement des données, le prétraitement et la construction du modèle CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2ad106",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow matplotlib scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5061783e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.utils import img_to_array, load_img\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ec5ca3",
   "metadata": {},
   "source": [
    "## Chargement et prétraitement des données\n",
    "\n",
    "Nous allons charger les images de spectrogrammes depuis le répertoire `data/05_cnn_input`, les prétraiter, puis les diviser en ensembles d'entraînement, de validation et de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f21e298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_dir = \"./../../data/05_cnn_input\"\n",
    "# data_dir = \"/tf/data/05_cnn_input\" \n",
    "\n",
    "# Dedecter si on est sous Windows ou Linux (Linux = Docker )\n",
    "if os.name == 'nt':\n",
    "    print(\"On est sous Windows.\")\n",
    "    data_dir = \"./../../data/05_cnn_input\"\n",
    "else:  # Ubuntu\n",
    "    print(\"On est sous Linux.\")\n",
    "    data_dir = \"/tf/data/05_cnn_input\"\n",
    "\n",
    "\n",
    "\n",
    "print(\"Contenu du dossier 05_cnn_input :\", os.listdir(data_dir))\n",
    "\n",
    "image_paths = []\n",
    "labels = []\n",
    "\n",
    "for label, category in enumerate(['balourd', 'sain']):\n",
    "    category_dir = os.path.join(data_dir, category)\n",
    "    if not os.path.exists(category_dir):\n",
    "        print(f\"Directory does not exist: {category_dir}\")\n",
    "        continue\n",
    "    print(f\"Checking directory: {category_dir}\")\n",
    "    for root, _, files in os.walk(category_dir):\n",
    "        print(f\"Found {len(files)} files in {root}\")\n",
    "        for file in files:\n",
    "            if file.endswith(\".png\"):\n",
    "                image_paths.append(os.path.join(root, file))\n",
    "                labels.append(label)\n",
    "\n",
    "print(f\"Total directories checked: {len(['balourd', 'sain'])}\")\n",
    "print(f\"Total image paths collected: {len(image_paths)}\")\n",
    "print(\"Sample image paths:\", image_paths[:5])\n",
    "print(\"Labels distribution:\", {label: labels.count(label) for label in set(labels)})\n",
    "\n",
    "image_paths = np.array(image_paths)\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f5ebd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Prétraitement des images : charger les images telles quelles (sans redimensionnement ni rognage)\n",
    "def preprocess_image(image_path):\n",
    "    try:\n",
    "        img = load_img(image_path)  # Charger l'image sans redimensionnement\n",
    "        img_array = img_to_array(img)\n",
    "        return img_array\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors du traitement de l'image {image_path} : {e}\")\n",
    "        return None\n",
    "\n",
    "# Débogage : Afficher les informations sur le jeu de données\n",
    "print(f\"Nombre total de chemins d'images trouvés : {len(image_paths)}\")\n",
    "print(\"Exemples de chemins d'images :\", image_paths[:5])\n",
    "print(\"Répartition des étiquettes :\", {label: labels.tolist().count(label) for label in set(labels)})\n",
    "\n",
    "# Appliquer le prétraitement à toutes les images\n",
    "images = np.array([img for img in (preprocess_image(path) for path in image_paths) if img is not None])\n",
    "\n",
    "# Débogage : Vérifier les images traitées\n",
    "print(f\"Nombre total d'images valides traitées : {len(images)}\")\n",
    "if len(images) > 0:\n",
    "    print(\"Dimensions de la première image :\", images[0].shape)\n",
    "\n",
    "# Diviser le jeu de données en ensembles d'entraînement, de validation et de test\n",
    "if len(images) == 0:\n",
    "    raise ValueError(\"Aucune image valide n'a été traitée. Veuillez vérifier le jeu de données.\")\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(images, labels, test_size=0.4, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Débogage : Afficher les répartitions des ensembles de données\n",
    "print(f\"Ensemble d'entraînement : {len(X_train)} échantillons\")\n",
    "print(f\"Ensemble de validation : {len(X_val)} échantillons\")\n",
    "print(f\"Ensemble de test : {len(X_test)} échantillons\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bedf5f0",
   "metadata": {},
   "source": [
    "## Construction du modèle CNN\n",
    "\n",
    "Nous allons définir une architecture de réseau de neurones convolutionnel (CNN) pour traiter les images de spectrogrammes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8d9e89da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_12 (Conv2D)          (None, 127, 99, 32)       896       \n",
      "                                                                 \n",
      " batch_normalization_12 (Ba  (None, 127, 99, 32)       128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " max_pooling2d_12 (MaxPooli  (None, 63, 49, 32)        0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " conv2d_13 (Conv2D)          (None, 61, 47, 64)        18496     \n",
      "                                                                 \n",
      " batch_normalization_13 (Ba  (None, 61, 47, 64)        256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " max_pooling2d_13 (MaxPooli  (None, 30, 23, 64)        0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " conv2d_14 (Conv2D)          (None, 28, 21, 128)       73856     \n",
      "                                                                 \n",
      " batch_normalization_14 (Ba  (None, 28, 21, 128)       512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " max_pooling2d_14 (MaxPooli  (None, 14, 10, 128)       0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " conv2d_15 (Conv2D)          (None, 12, 8, 256)        295168    \n",
      "                                                                 \n",
      " batch_normalization_15 (Ba  (None, 12, 8, 256)        1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " max_pooling2d_15 (MaxPooli  (None, 6, 4, 256)         0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " flatten_3 (Flatten)         (None, 6144)              0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 256)               1573120   \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2004673 (7.65 MB)\n",
      "Trainable params: 2003713 (7.64 MB)\n",
      "Non-trainable params: 960 (3.75 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Définir le modèle CNN\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(129, 101, 3)),  # Updated input shape\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(256, (3, 3), activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Afficher le résumé du modèle\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db346ae",
   "metadata": {},
   "source": [
    "## Entraînement du modèle et sauvegarde\n",
    "\n",
    "Nous allons compiler le modèle, l'entraîner en utilisant les ensembles d'entraînement et de validation, puis enregistrer le modèle entraîné avec une convention de nommage basée sur les performances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eda804d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "15/15 [==============================] - 3s 41ms/step - loss: 1.7633 - accuracy: 0.6192 - val_loss: 2.3482 - val_accuracy: 0.7107\n",
      "Epoch 2/20\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 1.5640 - accuracy: 0.6109 - val_loss: 0.7394 - val_accuracy: 0.4088\n",
      "Epoch 3/20\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 1.3731 - accuracy: 0.6172 - val_loss: 1.0331 - val_accuracy: 0.7107\n",
      "Epoch 4/20\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 1.0585 - accuracy: 0.6423 - val_loss: 0.5885 - val_accuracy: 0.7107\n",
      "Epoch 5/20\n",
      "15/15 [==============================] - 0s 20ms/step - loss: 0.9188 - accuracy: 0.6569 - val_loss: 0.6953 - val_accuracy: 0.4528\n",
      "Epoch 6/20\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.6934 - accuracy: 0.7259 - val_loss: 0.5723 - val_accuracy: 0.7862\n",
      "Epoch 7/20\n",
      "15/15 [==============================] - 0s 20ms/step - loss: 0.5386 - accuracy: 0.7615 - val_loss: 0.5357 - val_accuracy: 0.7673\n",
      "Epoch 8/20\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.3422 - accuracy: 0.8619 - val_loss: 0.3906 - val_accuracy: 0.8365\n",
      "Epoch 9/20\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.2694 - accuracy: 0.8975 - val_loss: 0.4247 - val_accuracy: 0.9119\n",
      "Epoch 10/20\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.1969 - accuracy: 0.9100 - val_loss: 0.8574 - val_accuracy: 0.4717\n",
      "Epoch 11/20\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.1393 - accuracy: 0.9414 - val_loss: 0.1825 - val_accuracy: 0.9434\n",
      "Epoch 12/20\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.1447 - accuracy: 0.9519 - val_loss: 0.3197 - val_accuracy: 0.9119\n",
      "Epoch 13/20\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.1008 - accuracy: 0.9644 - val_loss: 0.2588 - val_accuracy: 0.9308\n",
      "Epoch 14/20\n",
      "15/15 [==============================] - 0s 20ms/step - loss: 0.0419 - accuracy: 0.9895 - val_loss: 0.1341 - val_accuracy: 0.9434\n",
      "Epoch 15/20\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.0324 - accuracy: 0.9874 - val_loss: 0.1530 - val_accuracy: 0.9497\n",
      "Epoch 16/20\n",
      "15/15 [==============================] - 0s 20ms/step - loss: 0.0356 - accuracy: 0.9874 - val_loss: 0.1132 - val_accuracy: 0.9686\n",
      "Epoch 17/20\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.0174 - accuracy: 0.9937 - val_loss: 0.3120 - val_accuracy: 0.9245\n",
      "Epoch 18/20\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.0090 - accuracy: 0.9958 - val_loss: 0.3364 - val_accuracy: 0.9182\n",
      "Epoch 19/20\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.0186 - accuracy: 0.9916 - val_loss: 0.2005 - val_accuracy: 0.9497\n",
      "Epoch 20/20\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.0096 - accuracy: 0.9979 - val_loss: 0.1228 - val_accuracy: 0.9811\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Compiler le modèle\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Entraîner le modèle\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=20,\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "# Générer un nom de fichier avec un timestamp et les performances\n",
    "def generate_model_name(history, test_accuracy):\n",
    "    val_accuracy = max(history.history['val_accuracy']) * 100\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    return f\"{timestamp}_validation_res_{val_accuracy:.2f}_test_set_{test_accuracy:.2f}.h5\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca6e7b6",
   "metadata": {},
   "source": [
    "## Évaluation du modèle\n",
    "\n",
    "Nous allons évaluer le modèle entraîné sur l'ensemble de test et calculer la précision sur cet ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3a8cff2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0779 - accuracy: 0.9875\n",
      "Précision sur l'ensemble de test : 98.75%\n",
      "Modèle sauvegardé avec succès.\n"
     ]
    }
   ],
   "source": [
    "## Évaluation du modèle\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Précision sur l'ensemble de test : {test_accuracy * 100:.2f}%\")\n",
    "\n",
    "if test_accuracy * 100 > 97:\n",
    "    # Sauvegarder le modèle avec la précision mise à jour\n",
    "    model.save(generate_model_name(history, test_accuracy * 100))\n",
    "    print(\"Modèle sauvegardé avec succès.\")\n",
    "else:\n",
    "    print(\"La précision sur l'ensemble de test est inférieure à 98%. Le modèle ne sera pas sauvegardé.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d48854-f592-46a5-ad91-1ca8e750bb25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
