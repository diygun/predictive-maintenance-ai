{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4cd6ca6e",
   "metadata": {},
   "source": [
    "# Entraînement d'un CNN avec des images de spectrogrammes\n",
    "\n",
    "Ce notebook entraîne un réseau de neurones convolutionnel (CNN) en utilisant des images de spectrogrammes des vibrations du moteur. Le jeu de données est divisé en ensembles d'entraînement, de validation et de test. Le modèle entraîné est sauvegardé avec un horodatage et une convention de nommage basée sur les performances."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f09fe9",
   "metadata": {},
   "source": [
    "## Importation des bibliothèques (libraries) nécessaires\n",
    "\n",
    "Nous allons importer les bibliothèques nécessaires pour le chargement des données, le prétraitement et la construction du modèle CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5061783e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ec5ca3",
   "metadata": {},
   "source": [
    "## Chargement et prétraitement des données\n",
    "\n",
    "Nous allons charger les images de spectrogrammes depuis le répertoire `data/05_cnn_input`, les prétraiter, puis les diviser en ensembles d'entraînement, de validation et de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f21e298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contenu du dossier 05_cnn_input : ['balourd', 'sain', 'test', 'train', 'val']\n",
      "Checking directory: ./../../data/05_cnn_input/balourd\n",
      "Found 246 files in ./../../data/05_cnn_input/balourd\n",
      "Checking directory: ./../../data/05_cnn_input/sain\n",
      "Found 551 files in ./../../data/05_cnn_input/sain\n",
      "Total directories checked: 2\n",
      "Total image paths collected: 797\n",
      "Sample image paths: ['./../../data/05_cnn_input/balourd/spec_rgb_0551.png', './../../data/05_cnn_input/balourd/spec_rgb_0552.png', './../../data/05_cnn_input/balourd/spec_rgb_0553.png', './../../data/05_cnn_input/balourd/spec_rgb_0554.png', './../../data/05_cnn_input/balourd/spec_rgb_0555.png']\n",
      "Labels distribution: {0: 246, 1: 551}\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"./../../data/05_cnn_input\"\n",
    "\n",
    "print(\"Contenu du dossier 05_cnn_input :\", os.listdir(data_dir))\n",
    "\n",
    "image_paths = []\n",
    "labels = []\n",
    "\n",
    "for label, category in enumerate(['balourd', 'sain']):\n",
    "    category_dir = os.path.join(data_dir, category)\n",
    "    if not os.path.exists(category_dir):\n",
    "        print(f\"Directory does not exist: {category_dir}\")\n",
    "        continue\n",
    "    print(f\"Checking directory: {category_dir}\")\n",
    "    for root, _, files in os.walk(category_dir):\n",
    "        print(f\"Found {len(files)} files in {root}\")\n",
    "        for file in files:\n",
    "            if file.endswith(\".png\"):\n",
    "                image_paths.append(os.path.join(root, file))\n",
    "                labels.append(label)\n",
    "\n",
    "print(f\"Total directories checked: {len(['balourd', 'sain'])}\")\n",
    "print(f\"Total image paths collected: {len(image_paths)}\")\n",
    "print(\"Sample image paths:\", image_paths[:5])\n",
    "print(\"Labels distribution:\", {label: labels.count(label) for label in set(labels)})\n",
    "\n",
    "image_paths = np.array(image_paths)\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d6f5ebd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre total d'images valides traitées : 265\n",
      "Nombre total d'étiquettes correspondantes : 265\n",
      "Dimensions de la première image : (129, 101, 3)\n",
      "Ensemble d'entraînement : 159 échantillons\n",
      "Ensemble de validation : 53 échantillons\n",
      "Ensemble de test : 53 échantillons\n"
     ]
    }
   ],
   "source": [
    "# Appliquer le prétraitement pour fusionner les spectrogrammes\n",
    "images = []\n",
    "filtered_labels = []  # Nouvelle liste pour les étiquettes correspondantes\n",
    "\n",
    "for i in range(0, len(image_paths), 3):  # Supposer que les fichiers sont triés par moment\n",
    "    if i + 2 < len(image_paths):\n",
    "        merged_image = merge_spectrograms(image_paths[i], image_paths[i + 1], image_paths[i + 2])\n",
    "        if merged_image is not None:\n",
    "            images.append(merged_image)\n",
    "            filtered_labels.append(labels[i])  # Ajouter l'étiquette correspondante\n",
    "\n",
    "images = np.array(images)\n",
    "filtered_labels = np.array(filtered_labels)  # Convertir en tableau numpy\n",
    "\n",
    "# Débogage : Vérifier les images traitées\n",
    "print(f\"Nombre total d'images valides traitées : {len(images)}\")\n",
    "print(f\"Nombre total d'étiquettes correspondantes : {len(filtered_labels)}\")\n",
    "if len(images) > 0:\n",
    "    print(\"Dimensions de la première image :\", images[0].shape)\n",
    "\n",
    "# Diviser le jeu de données en ensembles d'entraînement, de validation et de test\n",
    "if len(images) == 0:\n",
    "    raise ValueError(\"Aucune image valide n'a été traitée. Veuillez vérifier le jeu de données.\")\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(images, filtered_labels, test_size=0.4, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Débogage : Afficher les répartitions des ensembles de données\n",
    "print(f\"Ensemble d'entraînement : {len(X_train)} échantillons\")\n",
    "print(f\"Ensemble de validation : {len(X_val)} échantillons\")\n",
    "print(f\"Ensemble de test : {len(X_test)} échantillons\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bedf5f0",
   "metadata": {},
   "source": [
    "## Construction du modèle CNN\n",
    "\n",
    "Nous allons définir une architecture de réseau de neurones convolutionnel (CNN) pour traiter les images de spectrogrammes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9e89da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_12 (Conv2D)          (None, 127, 99, 32)       896       \n",
      "                                                                 \n",
      " max_pooling2d_12 (MaxPooli  (None, 63, 49, 32)        0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " conv2d_13 (Conv2D)          (None, 61, 47, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_13 (MaxPooli  (None, 30, 23, 64)        0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " conv2d_14 (Conv2D)          (None, 28, 21, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_14 (MaxPooli  (None, 14, 10, 128)       0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " flatten_4 (Flatten)         (None, 17920)             0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 128)               2293888   \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2387265 (9.11 MB)\n",
      "Trainable params: 2387265 (9.11 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "\n",
    "# Définir le modèle CNN\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(129, 101, 3)),  # Updated input shape\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Afficher le résumé du modèle\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db346ae",
   "metadata": {},
   "source": [
    "## Entraînement du modèle et sauvegarde\n",
    "\n",
    "Nous allons compiler le modèle, l'entraîner en utilisant les ensembles d'entraînement et de validation, puis enregistrer le modèle entraîné avec une convention de nommage basée sur les performances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda804d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "5/5 [==============================] - 2s 317ms/step - loss: 110.9671 - accuracy: 0.5660 - val_loss: 5.0228 - val_accuracy: 0.6415\n",
      "Epoch 2/10\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 4.0660 - accuracy: 0.5975 - val_loss: 0.7262 - val_accuracy: 0.6415\n",
      "Epoch 3/10\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.9331 - accuracy: 0.6038 - val_loss: 0.6406 - val_accuracy: 0.6415\n",
      "Epoch 4/10\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 1.3242 - accuracy: 0.5849 - val_loss: 0.6859 - val_accuracy: 0.6226\n",
      "Epoch 5/10\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.6553 - accuracy: 0.6855 - val_loss: 0.6515 - val_accuracy: 0.6415\n",
      "Epoch 6/10\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.6794 - accuracy: 0.6730 - val_loss: 0.6541 - val_accuracy: 0.6415\n",
      "Epoch 7/10\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.6395 - accuracy: 0.6792 - val_loss: 0.6465 - val_accuracy: 0.6415\n",
      "Epoch 8/10\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.6674 - accuracy: 0.6792 - val_loss: 0.6428 - val_accuracy: 0.6415\n",
      "Epoch 9/10\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.6640 - accuracy: 0.6792 - val_loss: 0.6664 - val_accuracy: 0.6415\n",
      "Epoch 10/10\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.6605 - accuracy: 0.6792 - val_loss: 0.6467 - val_accuracy: 0.6415\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Compiler le modèle\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Entraîner le modèle\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=10,\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "# Générer un nom de fichier avec un timestamp et les performances\n",
    "def generate_model_name(history, test_accuracy):\n",
    "    val_accuracy = max(history.history['val_accuracy']) * 100\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    return f\"{timestamp}_validation_res_{val_accuracy:.2f}_test_set_{test_accuracy:.2f}.h5\"\n",
    "\n",
    "# Sauvegarder le modèle\n",
    "model.save(generate_model_name(history, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca6e7b6",
   "metadata": {},
   "source": [
    "## Évaluation du modèle\n",
    "\n",
    "Nous allons évaluer le modèle entraîné sur l'ensemble de test et calculer la précision sur cet ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8cff2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 6ms/step - loss: 0.6028 - accuracy: 0.7736\n",
      "Test Accuracy: 77.36%\n"
     ]
    }
   ],
   "source": [
    "## Évaluation du modèle\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Précision sur l'ensemble de test : {test_accuracy * 100:.2f}%\")\n",
    "\n",
    "# Sauvegarder le modèle avec la précision mise à jour\n",
    "model.save(generate_model_name(history, test_accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d48854-f592-46a5-ad91-1ca8e750bb25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
